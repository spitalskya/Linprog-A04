\documentclass[report.tex]{subfiles}

\begin{document}

\section{Úloha E}	

\subsection{Spracovanie všeobecnej triedy pre $L^1$ a $L^{\infty}$ regresiu}

Vypracovali sme modul \pyth|Model| pre počítanie $L^1$ a $L^{\infty}$ regresie z ľubovoľných číselných dát, ktorý využíva LP formulácie popísané vyššie. Konkrétne \pyth|L1Model| využíva formuláciu na minimalizovanie $L^1$ normy a \pyth|LInfModel| minimalizuje $L^{\infty}$ normu. Príklad použitia tohto modelu sa nachádza v \verb|model_demonstration.ipynb| Následne opíšeme jednotlivé metódy jednotlivých modelov.

\subsubsection*{\pyth|Model.__init__(dependent_vect, independent_vect)|}

Konštruktor triedy, spoločný pre oba modely, vytvorí inštanciu, ktorá si drží dáta a vie na nich vykonávať operácie popísané nižšie. 

Argumenty:

\begin{itemize}
	\item \pyth|dependent_vect: np.array| - vektor závislých premenných
	\item \pyth|independent_vect: np.array| - matica, ktorej stĺpce sú vektory nezávislých premenných
\end{itemize}

\subsubsection*{\pyth|Model.solve()|}

Metóda, ktorá vyrieši regresnú LP úlohu na daných dátach. \pyth|L1Model.solve()| rieši minimalizáciou $L^1$ normy a \pyth|LInfModel.solve()|, rieši minimalizáciou $L^{\infty}$ normy. 

Vracia:

\begin{itemize}
	\item \pyth|np.array| - vektor optimálnych $\beta$ premenných
\end{itemize}

Po zavolaní tejto metódy si inštancia uloží vektor optimálnych $\beta$ premenných do atribútu \pyth|self._beta|, potrebné pre metódy popísané nižšie.

\subsubsection*{\pyth|Model.r2()|}

Vypočíta $R^2$ koeficient pre dané dáta a vypočítaný vektor $\beta$.

Vracia:

\begin{itemize}
	\item \pyth|float| - výsledný $R^2$ koeficient
\end{itemize}

\subsubsection*{\pyth|Model.visualize()|}

Ak je počet nezávislých premenných 1 alebo 2, táto metóda vykreslí graf dát spolu s vypočítanou regresnou priamkou, resp. rovinou. 

Vracia:

\begin{itemize}
	\item \pyth|bool| - úspešnosť vizualizácie, kde \pyth|False| označuje, že nezávislých premenných je viac ako 2, čiže nie je možné vykresliť graf
\end{itemize}

%\subsection{Diskusia o vhodnotosti použitia $L^1$ a $L^{\infty}$ lineárnej regresie}
%
%\textit{Nasledujúca krátka sekcia popisuje len naše pozorovania, tvrdenia nie sú nami matematicky dokázané}
%
%Vyššie v sekcii \ref{sec:A} sme ukázali, že implementácie lineárnej regresie pomocou merania vzdialenost $L^1$ a $L^{\infty}$ normou majú optimálne riešenie, pre ľubovoľné vstupné dáta. Snažili sme sa odpozorovať, ako sa jednotlivé prístupy odlišujú pre nejaké konkrétne dáta.
%
%V dátach, v ktorých je výrazná lineárna závislosť, minimalizovanie $L^1$ normy veľmi dobre zachytáva práve tento lineárny vzťah, aj v prítomnosti outlierov. Toto správanie vie ale viesť aj k tzv. \textit{overfittingu}. Model príliš tesne zachytáva takéto správanie, čo môže viesť k horším odhadom pre budúce pozorovania.
%
%Na druhej strane minimalizovanie $L^{\infty}$ normy je veľmi ovplyvňované outliermi. Aj pre \enquote{jasne} lineárna dáta s jedným chybným pozorovaním, tento bod výrazne odkloní regresnú priamku/nadrovinu. To ale zároveň spôsobuje, že z našich pozorovaní (na 2D a 3D dátach) takáto regresia celkom dobre koreluje so 
%
%% Najznámejší spôsob počítania regresnej nadroviny je metóda najmenších štvorcov. Pri predpise $\beta = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$, zložitosť výpočtu má komplexitu $O(n^3)$, kde $n$ je počet pozorovaní (riadkov matice $\mathbf{X}$). 




\end{document}